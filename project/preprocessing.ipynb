{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this notebook\n",
    "This notebook contains our workflow to convert the million songs dataset with h5 format into panda pickle files.\n",
    "We convert our datasets into pickle in order to fasten the process of loading the dataset and avoid reading complex hadoop hdfs h5 files.\n",
    "We separate the datasets based on the raw datasets' directory structure, for example:\n",
    "> All raw h5 files in '/datasets/raw_files/A/' will be stored as a single file \"df_pickle_A\" in '/datasets/pickle_file/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import h5py\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset_dir    = \"./datasets/raw_files\"\n",
    "pickle_dataset_dir = \"./datasets/pickle_files\"\n",
    "\n",
    "# create alphabet letters since the raw datasets are arranged by alphabet letters\n",
    "letters            = string.ascii_uppercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pickle_datasets(letters):\n",
    "    for letter in letters:\n",
    "        df = pd.DataFrame()    \n",
    "        for subdir, dirs, files in os.walk(raw_dataset_dir + \"/\" + letter):\n",
    "            for file in files:\n",
    "                # avoid reading hidden files\n",
    "                if not file[0] == \".\":\n",
    "                    store = pd.HDFStore(subdir+'/'+file)\n",
    "                    df    = df.append(pd.concat([store['/analysis/songs'],store['/metadata/songs'],store['/musicbrainz/songs']],axis=1))\n",
    "                    store.close()\n",
    "        df.to_pickle(pickle_dataset_dir + \"/\" + \"df_pickle_\" + letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_pickle_datasets(letters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
